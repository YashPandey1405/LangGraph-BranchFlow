import "dotenv/config";
import { StateGraph, MessagesAnnotation } from "@langchain/langgraph";
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, AIMessage } from "@langchain/core/messages";

// Initialize OpenAI LLM
const llm = new ChatOpenAI({
  model: "gpt-4o-mini",
});

// Node 1: Input Logger
async function logInput(state) {
  const lastMsg = state.messages[state.messages.length - 1]?.content;
  console.log("📝 Input:", lastMsg);
  return { messages: state.messages };
}

// Node 2: Preprocess Input
async function preprocessInput(state) {
  const lastMsg = state.messages[state.messages.length - 1]?.content || "";
  const cleanMsg = lastMsg.trim().replace(/\s+/g, " ");
  console.log("✨ Cleaned:", cleanMsg);

  return {
    messages: [...state.messages, new HumanMessage(`(cleaned) ${cleanMsg}`)],
  };
}

// Node 3A: Call OpenAI (LLM)
async function callOpenAI(state) {
  console.log("🤖 Sending to OpenAI...");
  const response = await llm.invoke(state.messages);
  console.log("✅ LLM Response:", response.content);
  return { messages: [response] };
}

// Node 3B: Fun Fact Node (Branch)
async function funFactNode() {
  console.log("🎉 Using FunFact branch");
  return {
    messages: [new AIMessage("Here’s a fun fact: Honey never spoils 🍯")],
  };
}

// Node 4: Postprocess Output
async function postprocessOutput(state) {
  const lastMsg = state.messages[state.messages.length - 1];
  const updatedContent = `${lastMsg.content} \n\n(Note: This was generated by AI ✨)`;
  console.log("🎯 Finalized Output");
  return { messages: [new AIMessage(updatedContent)] };
}

// Node 5: Output Logger
async function logOutput(state) {
  const lastMsg = state.messages[state.messages.length - 1]?.content;
  console.log("📦 Final State:", lastMsg);
  return state;
}

// Create Workflow Graph
const workflow = new StateGraph(MessagesAnnotation)
  .addNode("logInput", logInput)
  .addNode("preprocessInput", preprocessInput)
  .addNode("callOpenAI", callOpenAI)
  .addNode("funFactNode", funFactNode)
  .addNode("postprocessOutput", postprocessOutput)
  .addNode("logOutput", logOutput)

  // Edges
  .addEdge("__start__", "logInput")
  .addEdge("logInput", "preprocessInput")

  // Branch: math vs non-math
  .addConditionalEdges(
    "preprocessInput",
    (state) => {
      const text = state.messages[state.messages.length - 1]?.content || "";
      return /\d|\+|\-|\*|\//.test(text) ? "math" : "nonMath";
    },
    {
      math: "callOpenAI",
      nonMath: "funFactNode",
    }
  )

  // Merge back
  .addEdge("callOpenAI", "postprocessOutput")
  .addEdge("funFactNode", "postprocessOutput")
  .addEdge("postprocessOutput", "logOutput")
  .addEdge("logOutput", "__end__");

// Compile workflow
const graph = workflow.compile();

// Run Graph
async function runGraph() {
  //   const userQuery = "Hey,   what   is  2  +   2  ?   "; // try "Tell me a joke" too
  const userQuery = "Tell me a joke";
  const updatedState = await graph.invoke({
    messages: [new HumanMessage(userQuery)],
  });
  console.log("🚀 Returned State:", updatedState.messages.at(-1).content);
}

runGraph();
